{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.3.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint data/Dataset/weights/TM_Vec/tm_vec_cath_model.ckpt`\n",
      "Embedding sequences:   8%|â–Š         | 531/6711 [00:40<07:50, 13.13it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-da0ef9ed7faa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0memb_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/Dataset/embeddings/Val_TM_Vec_test.npz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# Optionally, save the results to a file for later analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-da0ef9ed7faa>\u001b[0m in \u001b[0;36mfind_best_params\u001b[0;34m(seq_path, emb_path, max_seq_len)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmax_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmax_batch_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mtotal_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_residues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_residues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Tested max_residues={max_residues}, max_batch={max_batch}, time={total_time:.2f} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-da0ef9ed7faa>\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(seq_path, emb_path, max_residues, max_seq_len, max_batch)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mn_res_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_batch\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_res_batch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_residues\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mseq_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_sequences_tuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0membedded_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_deep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokeniser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0memb_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-da0ef9ed7faa>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(sequences, model_deep, model, tokenizer, device)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0membed_all_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mprotrans_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturize_prottrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprotrans_sequence\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\033[91mError: Could not embed sequence\\033[0m\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-da0ef9ed7faa>\u001b[0m in \u001b[0;36mfeaturize_prottrans\u001b[0;34m(sequences, model, tokenizer, device)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from tm_vec.embed_structure_model import trans_basic_block, trans_basic_block_Config\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "os.chdir('/home/ku76797/Documents/internship/Work/CATHe')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(\"Using device: {}\".format(device))\n",
    "\n",
    "def load_T5_model():\n",
    "    print(\"loading model\")\n",
    "    tokeniser = T5Tokenizer.from_pretrained(\"./data/Dataset/weights/ProtT5/prot_t5_xl_uniref50\", do_lower_case=False )\n",
    "    model = T5EncoderModel.from_pretrained(\"./data/Dataset/weights/ProtT5/prot_t5_xl_uniref50\")\n",
    "    gc.collect()\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "    \n",
    "    return model, tokeniser\n",
    "\n",
    "def read_csv(seq_path):\n",
    "    '''\n",
    "        Reads in CSV file containing sequences.\n",
    "        Returns a dictionary of sequences with IDs as keys.\n",
    "    '''\n",
    "    sequences = {}\n",
    "    df = pd.read_csv(seq_path)\n",
    "    for _ , row in df.iterrows():\n",
    "        sequences[str(row['Unnamed: 0'])] = row['Sequence']  # Ensure keys are strings\n",
    "    return sequences\n",
    "\n",
    "# Function to extract ProtTrans embedding for a sequence\n",
    "def featurize_prottrans(sequences, model, tokenizer, device):\n",
    "    sequences = [(\" \".join(seq)) for seq in sequences]\n",
    "    sequences = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences]\n",
    "    ids = tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding=\"longest\",)\n",
    "    input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            embedding = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    except RuntimeError:\n",
    "                print(\"RuntimeError during ProtT5 embedding  (nb sequences in batch={} /n (length of sequences in the batch ={}))\".format(len(sequences), [len(seq) for seq in sequences]))\n",
    "                sys.exit(\"Stopping execution due to RuntimeError.\")\n",
    "    \n",
    "    embedding = embedding.last_hidden_state.cpu().numpy()\n",
    "\n",
    "    features = []\n",
    "    for seq_num in range(len(sequences)):\n",
    "        seq_len = (attention_mask[seq_num] == 1).sum()\n",
    "        seq_emd = embedding[seq_num][:seq_len - 1]\n",
    "        features.append(seq_emd)\n",
    "\n",
    "    prottrans_embedding = torch.tensor(features[0])\n",
    "    prottrans_embedding = torch.unsqueeze(prottrans_embedding, 0).to(device)\n",
    "\n",
    "    return prottrans_embedding\n",
    "\n",
    "# Embed a protein using tm_vec (takes as input a prottrans embedding)\n",
    "def embed_tm_vec(prottrans_embedding, model_deep, device, seq):\n",
    "    padding = torch.zeros(prottrans_embedding.shape[0:2]).type(torch.BoolTensor).to(device)\n",
    "\n",
    "    try:\n",
    "        tm_vec_embedding = model_deep(prottrans_embedding, src_mask=None, src_key_padding_mask=padding)\n",
    "    \n",
    "    except RuntimeError:\n",
    "        print(\"RuntimeError during TM_Vec embedding sequence {}\".format(seq))\n",
    "        return None\n",
    "\n",
    "    return tm_vec_embedding.cpu().detach().numpy()\n",
    "\n",
    "def encode(sequences, model_deep, model, tokenizer, device):\n",
    "    embed_all_sequences = []\n",
    "    for seq in sequences:\n",
    "        protrans_sequence = featurize_prottrans([seq], model, tokenizer, device)\n",
    "        if protrans_sequence is None:\n",
    "            return None\n",
    "        embedded_sequence = embed_tm_vec(protrans_sequence, model_deep, device, seq)\n",
    "        embed_all_sequences.append(embedded_sequence)\n",
    "    return np.concatenate(embed_all_sequences, axis=0)\n",
    "\n",
    "def get_embeddings(seq_path, emb_path, max_residues, max_seq_len, max_batch):\n",
    "\n",
    "    emb_dict = dict()\n",
    "\n",
    "    # Read in CSV\n",
    "    sequences_dict = read_csv(seq_path)\n",
    "    sequences = list(sequences_dict.values())\n",
    "    sequence_keys = list(sequences_dict.keys())\n",
    "    \n",
    "    model, tokeniser = load_T5_model()\n",
    "\n",
    "    # TM-Vec model paths\n",
    "    tm_vec_model_cpnt = \"./data/Dataset/weights/TM_Vec/tm_vec_cath_model.ckpt\"\n",
    "    tm_vec_model_config = \"./data/Dataset/weights/TM_Vec/tm_vec_cath_model_params.json\"\n",
    "\n",
    "    # Load the TM-Vec model\n",
    "    tm_vec_model_config = trans_basic_block_Config.from_json(tm_vec_model_config)\n",
    "    model_deep = trans_basic_block.load_from_checkpoint(tm_vec_model_cpnt, config=tm_vec_model_config)\n",
    "    model_deep = model_deep.to(device)\n",
    "    model_deep = model_deep.eval()\n",
    "\n",
    "    sorted_sequences_tuple = sorted(zip(sequence_keys, sequences), key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    batch = []\n",
    "    batch_keys = []\n",
    "    for seq_idx, (seq_key, seq) in enumerate(tqdm(sorted_sequences_tuple, desc=\"Embedding sequences\"), 1):\n",
    "        seq_len = len(seq)\n",
    "        batch.append(seq)\n",
    "        batch_keys.append(seq_key)\n",
    "\n",
    "        n_res_batch = sum([len(s) for s in batch]) + seq_len\n",
    "        if len(batch) >= max_batch or n_res_batch >= max_residues or seq_idx == len(sorted_sequences_tuple) or seq_len > max_seq_len:\n",
    "            embedded_batch = encode(batch, model_deep, model, tokeniser, device)\n",
    "            for i, seq_key in enumerate(batch_keys):\n",
    "                emb_dict[seq_key] = embedded_batch[i]\n",
    "            batch = []\n",
    "            batch_keys = []\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    total_time = end - start\n",
    "    return total_time\n",
    "\n",
    "def find_best_params(seq_path, emb_path, max_seq_len=3263):\n",
    "    max_residues_values = [2**i for i in range(0, 42, 2)]  \n",
    "    max_batch_values = [2**i for i in range(0, 42, 2)]  \n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for max_residues in max_residues_values:\n",
    "        for max_batch in max_batch_values:\n",
    "            try:\n",
    "                total_time = get_embeddings(seq_path, emb_path, max_residues, max_seq_len, max_batch)\n",
    "                results.append((max_residues, max_batch, total_time))\n",
    "                print(f\"Tested max_residues={max_residues}, max_batch={max_batch}, time={total_time:.2f} seconds\")\n",
    "            except MemoryError:\n",
    "                results.append((max_residues, max_batch, \"Memory Error\"))\n",
    "                print(f\"Memory Error for max_residues={max_residues}, max_batch={max_batch}\")\n",
    "            except Exception as e:\n",
    "                results.append((max_residues, max_batch, f\"Error: {e}\"))\n",
    "                print(f\"Failed max_residues={max_residues}, max_batch={max_batch} with error: {e}\")\n",
    "    \n",
    "    # Find the best parameters\n",
    "    valid_results = [result for result in results if isinstance(result[2], (int, float))]\n",
    "    if valid_results:\n",
    "        best_params = min(valid_results, key=lambda x: x[2])\n",
    "        print(f\"Best parameters: max_residues={best_params[0]}, max_batch={best_params[1]} with time={best_params[2]:.2f} seconds\")\n",
    "    else:\n",
    "        print(\"No valid parameter combinations found.\")\n",
    "    \n",
    "    return results, best_params if valid_results else None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    seq_path = \"./data/Dataset/csv/Val.csv\"\n",
    "    emb_path = \"./data/Dataset/embeddings/Val_TM_Vec_test.npz\"\n",
    "    \n",
    "    results, best_params = find_best_params(seq_path, emb_path)\n",
    "    \n",
    "    # Optionally, save the results to a file for later analysis\n",
    "    results_df = pd.DataFrame(results, columns=[\"max_residues\", \"max_batch\", \"time\"])\n",
    "    results_df.to_csv(\"./src/all/models/TM_Vec/embedding_time_results.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
